\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amscd}
\usepackage{amsfonts}
\usepackage{dsfont}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{hyperref}


\newcommand{\ket}[1]{|#1\rangle}
\newcommand{\bra}[1]{\langle#1|}
\newcommand{\ketbra}[2]{| #1 \rangle \langle #2 |}
\newcommand{\braket}[2]{\langle  #1 |#2 \rangle}


%THEOREM STYLES
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{exerc}{Exercise}
\newtheorem{ans}{Answer}

%NICE HEADER
\pagestyle{fancy}\lhead{\textcolor{red}{}} \rhead{CQ - 2025/1}
\chead{{\large{\bf }}} \lfoot{} \rfoot{\bf \thepage} 
\cfoot{}

\newcounter{list}

\begin{document}

\begin{center}
\Large \textbf{{Assignment 1 - due March 30th}}
\end{center} 


\begin{ans}
\begin{enumerate}[(a)]
 	\item We will prove that $U^{*}U = I$. We know $U$ is diagonalizable, so $U$ can be written as \[U = \sum_{i=0}^{n} \lambda_i \ketbra{v_i}{v_i}\] where
			all $\lambda_i$ are eigenvalues and $\ket{v_i}$ are there respective eigenvectors. 
			Then, \[ U^{*} = \sum_{i=0}^{n} \overline{\lambda_i} \braket{v_i}{v_i}\]
			Note that all eigenvectors are orthogonal, each means that, $\braket{v_i}{v_i} \ketbra{v_j}{v_j} = 0$ when $i \neq j$, and $\braket{v_i}{v_i} \ketbra{v_j}{v_j} = 1$
			when $i = j$. So we have, \[ U^{*}U = \sum_{i=0}^{n} \overline{\lambda_i} \lambda_i\] and by the hypothesis
			$\overline{\lambda_i} \lambda_i = \left\lvert \lambda_i \right\rvert = 1$. Therefore $ U^{*}U = I$.
 	\item Take $\lambda$ a eigenvalue of U unitary, and $\ket{v}$ his associated eigenvector, so the equity bellow
			is true, \[ U \ket{v} = \lambda\ket{v} \]
			Note that, $ = \overline{\lambda}\bra{v}$.
			Then, when we multiply the first equation for $\bra{v}U^{*}$ on the left, we get 
			\[ \bra{v}U^{*} U \ket{v} = \bra{v}U^{*} \lambda \ket{v}\]
			\[ \braket{v}{v} = \bra{v}\overline{\lambda}\lambda \ket{v}\]
			\[ \braket{v}{v} = \overline{\lambda}\lambda \braket{v}{v}\]
			By the property of the inner product, we have that $\braket{v}{v} \neq 0$, so we can divide the last
			equation on both sides for $\braket{v}{v}$. And then, 
			\[ 1 = \overline{\lambda}\lambda = \left\lvert \lambda \right\rvert\]
 	\item blank
 \end{enumerate}
 
\end{ans}

\noindent \hrulefill

\begin{ans}
	Unfortunately I couldn't do the first part, but the second one is so simple that I feel bad not writing it.
	Assume $M \ket{v} = 0$, then
	\[ \bra{v} M \ket{v} = \bra{v} 0 = 0\]
	Sorry.
\end{ans}

\noindent \hrulefill

\begin{ans}
	First, we will show that $A\otimes I$ and $I\otimes B$ are diagonalizable.
	Since $A$ is diagonalizable, we can write $A$ as $A = PDP^{-1}$, where $D$ is a diagonal matrix and $P$ is invertible.
	Then, by the properties of the kronecker product 
	\[ A \otimes I = (PDP^{-1}) \otimes I = (P\otimes I)(D\otimes I)(P^{-1}\otimes I) = (P\otimes I)(D\otimes I)(P\otimes I)^{-1} \]
	Is easy to see that, since $D$ and $I$ are diagonal $D \otimes I$ is also diagonal. So now we've found a diagonalizable representation of $A \otimes I$ 
	, with $P\otimes I$ invertible and $D \otimes I$ diagonal.
	By a analogous reasoning, is easy to see that $I \otimes B$ is also diagonalizable, just noticing that if
	$B = QEQ^{-1}$, $I \otimes E$ is diagonal.
	Let P be a invertible matrix, since $I$ is the identity matrix, we can comute it in the product. So we have $P^{-1}IP = IP^{-1}P = I $.
	Now, to show that $A \otimes I+I \otimes B$ is diagonalizable, we need to find a matrix that diagonalizes it.
	Let $P\otimes Q$ be this candidate.
	\begin{flalign*}
		(P\otimes Q)^{-1}(A \otimes I+I \otimes B)(P\otimes Q) &= (P\otimes Q)^{-1}(A\otimes I)(P\otimes Q)+(P\otimes Q)^{-1}(I\otimes B)(P\otimes Q)&\\
		&= (P^{-1}AP) \otimes (Q^{-1}IQ) + (P^{-1}IP) \otimes (Q^{-1}BQ)&\\
		&= D \otimes I + I \otimes E
	\end{flalign*}
	Since $D \otimes I$ and $I \otimes E$ are diagonal, their sum is also diagonal.
	And then $A \otimes I+I \otimes B$ is diagonalizable.
	Now, we know $D$ and $E$ are the diagonal matrixes with the eigenvalues for $A$ and $B$ respectivly. 
	Note that $D \otimes I$ and $I \otimes E$ are diagonal matrixes with the same eigenvalues of $D$ and $E$,
	because the kronecker product, now the eigenvalues have multiplicity of the "size" of the square matrixes.
	So if the eigenvalues of $A$ are $\lambda_i$ and the eigenvalues of $B$ are $\delta_i$,
	then the eigenvalues of $A \otimes I+I \otimes B$ are $\lambda_i + \delta_i$.
\end{ans}

\noindent \hrulefill

\begin{ans}
	blank
\end{ans}

\noindent \hrulefill

\begin{ans}
	\begin{enumerate}[(a)]
		\item Assume $P_1 + P_2$ is an orthogonal projection. We have
			\[ (P_1 + P_2)^2 = (P_1)^2 + 2P_1P_2 + (P_2)^2 \]
			Note that, we know from the classes that, if M is a orthogonal projection, then $M^2 = M$.
			Since $P_1$ and $P_2$ are orthogonal projections, we have
			\begin{align*}
				(P_1)^2 + 2P_1P_2 + (P_2)^2 &= P_1 + P_2&\\
				P_1 + 2P_1P_2 + P_2 &= P_1 + P_2&\\
				2P_1P_2 &= 0&\\
				P_1P_2 &= 0
			\end{align*}
			Now assume that $P_1P_2 = 0$. To prove that $P_1 + P_2$ is a orthogonal projection,
			we need to verify that $(P_1 + P_2)^2 = P_1 + P_2$ and $(P_1 + P_2)^{*}=P_1 + P_2$.
			Since $P_1$ and $P_2$ are orthogonal projections, we have
			\begin{align*}
				(P_1 + P_2)^2 &= (P_1)^2 + 2P_1P_2 + (P_2)^2&\\
				&= P_1 + 0 + P_2&\\
				&= P_1 + P_2
			\end{align*}
			and
			\[ (P_1 + P_2)^{*} = (P_1)^{*} + (P_2)^{*} = P_1 + P_2\]
		\item We will use an analogous reasoning from the last item.
			Assume that $(P_1 + \dots + P_k)$ is an orthogonal projection. So is true that
			$(P_1 + \dots + P_k)^2 = (P_1 + \dots + P_k)$.
			But from the other hand, since for all $i$, $P_i$ is an orthogonal projection, we have
			\begin{align*}
				(P_1 + \cdots + P_k)^2 &= \sum_{i=1}^{k} (P_i)^2 + 2\sum_{1 \leq i < j \leq k}P_iP_j&\\
				&= \sum_{i=1}^{k} P_i  + 2\sum_{1 \leq i < j \leq k}P_iP_j&\\
			\end{align*}
			Then, because $(P_1 + \dots + P_k)$ is an orthogonal projection
			\begin{align*}
				\sum_{i=1}^{k} P_i  + 2\sum_{1 \leq i < j \leq k}P_iP_j &= \sum_{i=1}^{k} P_i&\\
				\sum_{1 \leq i < j \leq k}P_iP_j &= 0&\\
			\end{align*}
			Again, since for all $i$, $P_i$ is an orthogonal projection, we have that $P_iP_j$
			are positive semi-definite operators, $P_iP_j \geq 0$. Thus
			\[ \sum_{1 \leq i < j \leq k}P_iP_j = 0 \Rightarrow P_iP_j = 0 \quad \forall i \neq j \]
			Now assume, that $ P_iP_j = 0 \quad \forall i \neq j$. Similarly from the last item:
			\begin{align*}
				(P_1 + \cdots + P_k)^2 &= \sum_{i=1}^{k} (P_i)^2 + 2\sum_{1 \leq i < j \leq k}P_iP_j&\\
				&= \sum_{i=1}^{k} P_i  + 0&\\
				&= (P_1 + \cdots + P_k)&\\
			\end{align*}
			and also
			\begin{align*}
				(P_1 + \cdots + P_k)^{*} &= (P_1)^{*} + \dots + (P_k)^{*}&\\
				&= P_1 + \cdots + P_k
			\end{align*}
	\end{enumerate}
\end{ans}

\noindent \hrulefill

\begin{ans}
	blank
\end{ans}

\end{document}

